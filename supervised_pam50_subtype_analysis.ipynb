{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 🧠 Supervised Learning for Breast Cancer Subtype and Clinical Outcome Prediction\n",
    "\n",
    "This notebook demonstrates a supervised learning pipeline to classify breast cancer subtypes (PAM50) using gene expression data. The same framework can be adapted to predict clinical outcomes such as survival events by changing the input features `X` and target labels `y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Objectives\n",
    "\n",
    "- Build models to predict **PAM50 molecular subtypes** using gene expression data.\n",
    "- Evaluate multiple supervised learning methods:\n",
    "  - Random Forest\n",
    "  - Logistic Regression\n",
    "  - Deep Neural Networks (MLP)\n",
    "- Use performance metrics like:\n",
    "  - Classification report\n",
    "  - Confusion matrix\n",
    "  - Per-class and average accuracy\n",
    "- Extend the framework to **clinical outcome prediction** (e.g. survival binary classification).\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfb3b4e3236d34fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ⚙️ Feature and Label Preparation\n",
    "\n",
    "- `X`: Subset of informative genes (e.g. `limma_top_genes`) or dimensionality-reduced expression data.\n",
    "- `y`:  \n",
    "  - For subtype classification: `pam50_subtype`  \n",
    "  - For clinical prediction: `survival_binary` (e.g., long-term survivor vs. early death)\n",
    "\n",
    "```python\n",
    "# Example\n",
    "X = df_for_clustering.loc[:, limma_top_genes]\n",
    "y = df_zscore_meta[\"pam50_subtype\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e004606ae01074"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_meta_all = pd.read_csv(\"./data/df_meta.tsv\",sep=\"\\t\",index_col=0)\n",
    "df_zscore = pd.read_csv(\"./data/df_merged.tsv\",sep=\"\\t\",index_col=0)\n",
    "df_all = pd.read_csv(\"./data/GSE96058_gene_expression_3273_samples_and_136_replicates_transformed.csv\",sep=\",\",index_col=0)\n",
    "df_zscore_meta = df_meta_all[df_meta_all[\"sample_id\"].isin(list(df_zscore.columns))]\n",
    "df_for_clustering = df_zscore.T "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f004f2766b75c44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_for_clustering\n",
    "\n",
    "# # Assume df_zscore_meta is your metadata DataFrame\n",
    "# df_zscore_meta['survival_binary'] = np.nan  # Initialize as NaN\n",
    "#\n",
    "# # Long-term survivors: survival > 5 years (1825 days) and no death event\n",
    "# df_zscore_meta.loc[(df_zscore_meta['overall_survival_days'] > 1825) & (df_zscore_meta['overall_survival_event'] == 0), 'survival_binary'] = 1\n",
    "#\n",
    "# # Short-term death: survival < 3 years (1095 days) and death occurred\n",
    "# df_zscore_meta.loc[(df_zscore_meta['overall_survival_days'] < 1095) & (df_zscore_meta['overall_survival_event'] == 1), 'survival_binary'] = 0\n",
    "\n",
    "y = df_zscore_meta[\"pam50_subtype\"]\n",
    "\n",
    "# Encode subtype labels into integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# === Step 2: Split training and test sets ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbb00e00c192c1b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e0b313a29eeb4a3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🎯 Model A: Random Forest Classifier\n",
    "\n",
    "- **Type**: Ensemble Tree-based Classifier  \n",
    "- **Key Features**:\n",
    "  - Handles high-dimensional data and nonlinear relationships.\n",
    "  - Built-in feature selection through variable importance.\n",
    "  - Robust to overfitting (especially with many trees).\n",
    "  - Supports `class_weight=\"balanced\"` to handle class imbalance.\n",
    "\n",
    "- **Hyperparameters**:\n",
    "  - `n_estimators=200`: Number of trees\n",
    "  - `random_state=42`: Ensures reproducibility\n",
    "\n",
    "- **Evaluation**:\n",
    "  - Classification report\n",
    "  - Normalized confusion matrix\n",
    "  - Per-class accuracy\n",
    "  - Average accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e3a04c41d3004f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === 3. Model A: Random Forest ===\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"🎯 Random Forest Results:\")\n",
    "\n",
    "# Classification report — ensure target_names are strings\n",
    "target_names = [str(cls) for cls in np.unique(y_train)]  # Or use le.classes_ if LabelEncoder was used\n",
    "\n",
    "# Print classification evaluation report\n",
    "print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = le.classes_\n",
    "cm = confusion_matrix(y_test, y_pred_rf, normalize='true')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "# plt.title('Random Forest on PHATE Expression Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = np.diag(cm) / cm.sum(axis=1)\n",
    "\n",
    "# Average accuracy\n",
    "average_accuracy = np.mean(per_class_acc)\n",
    "print(average_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1991cfc5816a9f9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 📊 Model B: Logistic Regression (L2-regularized)\n",
    "\n",
    "- **Type**: Linear Model with Multinomial Softmax\n",
    "- **Key Features**:\n",
    "  - Fast and interpretable\n",
    "  - Coefficients reflect importance of features\n",
    "  - Supports multiclass settings\n",
    "  - Can regularize with L2 (Ridge) to avoid overfitting\n",
    "\n",
    "- **Hyperparameters**:\n",
    "  - `max_iter=1000`: Ensure convergence\n",
    "  - `solver='lbfgs'`: Suitable for multinomial loss\n",
    "  - `multi_class='auto'`: Automatically handles multiclass\n",
    "\n",
    "- **Model Interpretation**:\n",
    "  - Coefficients (`coef_`) are extracted and sorted by magnitude.\n",
    "  - Top features can be linked to known subtype markers.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b968710e7695ad1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === 4. Model B: Logistic Regression (with L2 regularization) ===\n",
    "logit = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred_logit = logit.predict(X_test)\n",
    "\n",
    "# Define class label names — use le.classes_ if LabelEncoder was used\n",
    "target_names = [str(cls) for cls in np.unique(y_train)]\n",
    "\n",
    "print(\"\\n📊 Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_logit, target_names=target_names))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "class_names = le.classes_\n",
    "cm = confusion_matrix(y_test, y_pred_logit, normalize='true')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "# plt.title('Logistic Regression on Raw Expression Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = np.diag(cm) / cm.sum(axis=1)\n",
    "\n",
    "# Average accuracy across classes\n",
    "average_accuracy = np.mean(per_class_acc)\n",
    "print(average_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c59ef28bddaefc3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get coefficients from the logistic regression model\n",
    "coefficients = logit.coef_[0]   # For multiclass, model.coef_ is a 2D array\n",
    "\n",
    "# Retrieve feature names if X is a DataFrame\n",
    "feature_names = X.columns       \n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Compute absolute coefficient values for ranking\n",
    "coef_df['AbsCoef'] = coef_df['Coefficient'].abs()\n",
    "\n",
    "# Sort features by absolute coefficient magnitude (descending)\n",
    "coef_df = coef_df.sort_values(by='AbsCoef', ascending=False)\n",
    "\n",
    "# Display the sorted coefficient table\n",
    "print(coef_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c35d57002b7b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🤖 Model C: Feedforward Neural Network (MLP)\n",
    "\n",
    "- **Type**: Multi-layer Perceptron using Keras\n",
    "- **Structure**:\n",
    "  - Input layer → Dense(512) → Dropout\n",
    "  - → Dense(256) → Dense(128) → Dropout\n",
    "  - → Output layer with Softmax for multiclass classification\n",
    "- **Highlights**:\n",
    "  - Captures complex nonlinear patterns in gene expression data\n",
    "  - Dropout used to reduce overfitting\n",
    "  - Class weights are used to balance subtype frequency\n",
    "- **Training**:\n",
    "  - Loss: categorical crossentropy\n",
    "  - Optimizer: Adam\n",
    "  - Evaluation: accuracy and classification report on validation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9a244ce2cbc6b44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assume you have already prepared:\n",
    "# X_expr: (3238, 16531) gene expression data\n",
    "# X_clinical: (3238, 28) clinical feature data\n",
    "# y: (3238,) multi-class labels, integer encoded\n",
    "# Example: clinical_df is your clinical data DataFrame\n",
    "\n",
    "# Use overall survival event as the target\n",
    "y = df_zscore_meta[\"overall_survival_event\"]\n",
    "\n",
    "# Number of classes (usually 2: event = 0 or 1)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Encode labels as integers\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_cat = to_categorical(y_int, num_classes)\n",
    "\n",
    "# Use the transposed z-score normalized expression matrix as input features\n",
    "X_expr = df_zscore.T\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ce9cffcea61e07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# === Define expression input model ===\n",
    "expr_input = Input(shape=(16531,), name=\"expr_input\")\n",
    "\n",
    "# Fully connected layers for expression input\n",
    "x_expr = Dense(512, activation='relu')(expr_input)\n",
    "x_expr = Dropout(0.3)(x_expr)\n",
    "x_expr = Dense(256, activation='relu')(x_expr)\n",
    "\n",
    "# Combine (only expression here; can be extended with clinical input later)\n",
    "x = x_expr\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer for classification\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=expr_input, outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Automatically compute class weights to address class imbalance\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y),\n",
    "                                     y=y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# === Train/Validation split ===\n",
    "X_expr_train, X_expr_val, y_train, y_val = train_test_split(\n",
    "    X_expr, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Model training ===\n",
    "model.fit(X_expr_train,\n",
    "          y_train,\n",
    "          validation_data=(X_expr_val, y_val),\n",
    "          epochs=50,\n",
    "          class_weight=class_weights,\n",
    "          batch_size=64)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = model.predict(X_expr_val)\n",
    "y_pred_label = np.argmax(y_pred, axis=1)\n",
    "y_true_label = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(classification_report(y_true_label, y_pred_label))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59bab6c40b745d55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
