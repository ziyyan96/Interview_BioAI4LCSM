{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸ§¬ Dimensionality Reduction and Unsupervised Clustering for PAM50 Subtypes\n",
    "\n",
    "This notebook explores a variety of **dimensionality reduction** and **unsupervised clustering** techniques applied to breast cancer gene expression data. The primary goal is to evaluate whether unsupervised methods can recover the known **PAM50 subtypes**.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a32737b430424e35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f69a8dfee3c044"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_meta_all = pd.read_csv(\"./data/df_meta.tsv\",sep=\"\\t\",index_col=0)\n",
    "df_zscore = pd.read_csv(\"./data/df_merged.tsv\",sep=\"\\t\",index_col=0)\n",
    "df_zscore_meta = df_meta_all[df_meta_all[\"sample_id\"].isin(list(df_zscore.columns))]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ffd19fa09e4cee9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“‰ Step 2: Dimensionality Reduction Methods\n",
    "\n",
    "Several dimensionality reduction techniques are used to project high-dimensional expression data into low-dimensional space.\n",
    "\n",
    "### 2.1 Principal Component Analysis (PCA)\n",
    "\n",
    "- A linear projection method that captures the largest variance directions.\n",
    "- Used for both visualization and feature extraction.\n",
    "- 2D PCA plots help visually assess subtype separability."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d29e99c72e226b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assume df_merged is the processed expression matrix (genes as rows, samples as columns); transpose it\n",
    "df_for_clustering = df_zscore.T  # After transposing, each row is a sample\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_for_clustering)\n",
    "\n",
    "# Subtype labels (for coloring)\n",
    "subtypes = df_zscore_meta[\"pam50_subtype\"]\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=pca_result[:, 0], \n",
    "    y=pca_result[:, 1],\n",
    "    hue=subtypes,\n",
    "    hue_order=[\"Basel\", \"LumA\", \"LumB\", \"Normal\", \"Her2\"]\n",
    ")\n",
    "\n",
    "plt.title('PCA on Processed Gene Expression')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bcded78a3744e71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Autoencoder\n",
    "\n",
    "- A neural network trained to reconstruct input data through a low-dimensional \"bottleneck\" layer.\n",
    "- Encoder output is used as compressed features.\n",
    "- Captures non-linear structure in gene expression.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844a685f92c3f2b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try dimension reduction using a simple autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Assume df_for_clustering is the expression matrix with shape (n_samples, n_genes)\n",
    "input_dim = df_for_clustering.shape[1]\n",
    "encoding_dim = 2  # Reduce to 2D for visualization\n",
    "\n",
    "# Define the autoencoder structure\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Define full autoencoder and encoder models\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Compile and train the autoencoder\n",
    "autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "autoencoder.fit(df_for_clustering, df_for_clustering, epochs=50, batch_size=64, shuffle=True)\n",
    "\n",
    "# Get the 2D encoded representation\n",
    "X_encoded = encoder.predict(df_for_clustering)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b27f859646b55c64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸŒ PHATE for Nonlinear Dimensionality Reduction\n",
    "\n",
    "PHATE (Potential of Heat-diffusion for Affinity-based Transition Embedding) is a nonlinear dimensionality reduction technique especially well-suited for high-dimensional biological data such as gene expression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34c5f6b6155e0dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import phate\n",
    "\n",
    "phate_operator = phate.PHATE(n_components=2, knn=5)\n",
    "X_phate = phate_operator.fit_transform(df_for_clustering)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=X_phate[:,0], y=X_phate[:,1])\n",
    "plt.title(\"PHATE projection of expression\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e9492f5ce88580"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 SimCLR for Gene Expression\n",
    "\n",
    "- Contrastive self-supervised learning adapted to gene expression data.\n",
    "- Uses gene masking and Gaussian noise as augmentations.\n",
    "- A projection head is trained using NT-Xent loss.\n",
    "- Learned 128-dimensional embeddings are used for clustering.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c59f3a870c70e670"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ----------------- expression matrix -----------------\n",
    "#    X : (n_samples, n_genes)  â€“ already z-scored / log-scaled\n",
    "X = np.array(df_for_clustering)                               # â† your numpy array\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# ----------------- define two augmentations ------------\n",
    "def aug_dropout(x, p=0.1):\n",
    "    mask = (torch.rand_like(x) > p).float()\n",
    "    return x * mask\n",
    "\n",
    "def aug_gaussian_noise(x, sigma=0.15):\n",
    "    return x + sigma * torch.randn_like(x)\n",
    "\n",
    "class ExpressionPairDataset(TensorDataset):\n",
    "    \"\"\"Return two differently-augmented views of the same sample.\"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tensors[0][idx]\n",
    "        return aug_dropout(x), aug_gaussian_noise(x)\n",
    "\n",
    "pair_ds   = ExpressionPairDataset(X_tensor)\n",
    "pair_dl   = DataLoader(pair_ds, batch_size=256, shuffle=True, drop_last=True)\n",
    "import torch.nn as nn\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "class SimCLRNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_feat=2, dim_out=2):\n",
    "        super().__init__()\n",
    "        # backbone : maps input â†’ feature (latent)  âŸµ ä½ æœ€ç»ˆæƒ³è¦çš„è¡¨ç¤º\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(dim_in, 512), nn.ReLU(),\n",
    "            nn.Linear(512, dim_feat)\n",
    "        )\n",
    "        # projection head : feature â†’ contrastive space\n",
    "        self.projection = SimCLRProjectionHead(dim_feat, dim_feat, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        z = self.projection(h)\n",
    "        return z\n",
    "\n",
    "model = SimCLRNet(dim_in=X.shape[1])\n",
    "loss_fn = NTXentLoss()                 # temperature-scaled InfoNCE\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for x1, x2 in pair_dl:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        z1, z2 = model(x1), model(x2)\n",
    "        loss   = loss_fn(z1, z2)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch:02d}  loss {total_loss/len(pair_dl):.4f}\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent = model.backbone(X_tensor.to(device)).cpu().numpy()   # (n_samples, 128)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e6defbeb62885e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "# ------------ 1. Data Preparation --------------------------------------------------\n",
    "X = np.asarray(df_for_clustering, dtype=np.float32)        # (n_samples, n_genes)\n",
    "X = StandardScaler().fit_transform(X)                      # Apply z-score normalization\n",
    "\n",
    "# (Optional) If PAM50 labels are available: used for balanced sampling\n",
    "y = df_zscore_meta[\"pam50_subtype\"].values                # String labels\n",
    "label2idx = {lab: i for i, lab in enumerate(np.unique(y))}\n",
    "y_int = np.array([label2idx[v] for v in y])\n",
    "\n",
    "tensor_X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# ------------ 2. Balanced Sampler --------------------------------------------------\n",
    "counts = Counter(y_int)\n",
    "class_weights = torch.tensor([1.0 / counts[i] for i in counts], dtype=torch.float32)\n",
    "sample_weights = class_weights[y_int]\n",
    "sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                num_samples=len(sample_weights),\n",
    "                                replacement=True)\n",
    "\n",
    "# ------------ 3. Expression-specific Data Augmentation -----------------------------\n",
    "def aug_mask(x, p=0.2):          # Randomly mask gene features\n",
    "    mask = (torch.rand_like(x) > p).float()\n",
    "    return x * mask\n",
    "\n",
    "def aug_gaussian(x, sigma=0.1):  # Add Gaussian noise\n",
    "    return x + sigma * torch.randn_like(x)\n",
    "\n",
    "class ExprPair(TensorDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tensors[0][idx]\n",
    "        return aug_mask(x), aug_gaussian(x)\n",
    "\n",
    "pair_ds = ExprPair(tensor_X)\n",
    "pair_dl = DataLoader(pair_ds, batch_size=256, sampler=sampler, drop_last=True, num_workers=0)\n",
    "\n",
    "# ------------ 4. SimCLR Network ----------------------------------------------------\n",
    "class SimCLRNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_feat=128, dim_out=128):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(dim_in, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 512), nn.ReLU(),\n",
    "            nn.Linear(512, dim_feat)\n",
    "        )\n",
    "        self.projection = SimCLRProjectionHead(dim_feat, dim_feat, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        z = self.projection(h)\n",
    "        return z\n",
    "\n",
    "# Initialize model\n",
    "model = SimCLRNet(dim_in=X.shape[1]).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss_fn = NTXentLoss(temperature=0.2)      # Lower temperature for better separation\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=50)\n",
    "\n",
    "# ------------ 5. Training ----------------------------------------------------------\n",
    "device = next(model.parameters()).device\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x1, x2 in pair_dl:\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        z1, z2 = model(x1), model(x2)\n",
    "        loss = loss_fn(z1, z2)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch:02d}  loss {epoch_loss/len(pair_dl):.4f}\")\n",
    "\n",
    "# ------------ 6. Extract Latent Representations ------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent = model.backbone(tensor_X.to(device)).cpu().numpy()   # (n_samples, 128)\n",
    "\n",
    "# `latent` can now be used for downstream clustering (e.g. KMeans, GMM, Spectral)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20858383246bf265"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ”— Step 3: Unsupervised Clustering Methods\n",
    "\n",
    "The latent features are clustered using various methods:\n",
    "\n",
    "### 3.1 K-Means\n",
    "\n",
    "- Simple and efficient.\n",
    "- Requires predefining the number of clusters.\n",
    "- Assumes spherical clusters.\n",
    "\n",
    "### 3.2 Gaussian Mixture Models (GMM)\n",
    "\n",
    "- A probabilistic clustering model.\n",
    "- Learns soft assignments based on multivariate Gaussians.\n",
    "\n",
    "### 3.3 Spectral Clustering\n",
    "\n",
    "- Constructs an affinity graph from samples.\n",
    "- Clustering is performed in the eigenvector space.\n",
    "- Captures complex, non-linear structures.\n",
    "\n",
    "### 3.4 HDBSCAN\n",
    "\n",
    "- Density-based clustering.\n",
    "- Automatically infers number of clusters.\n",
    "- Handles outliers and noise effectively.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e255b18f47f89445"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_biclusters\n",
    "from sklearn.cluster import SpectralBiclustering, SpectralCoclustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# -------------------------------------\n",
    "# Dimensionality Reduction using PCA\n",
    "# -------------------------------------\n",
    "# Transpose the z-score normalized matrix: samples as rows\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(df_zscore.T)\n",
    "\n",
    "# -------------------------------------\n",
    "# KMeans Clustering (Optional)\n",
    "# -------------------------------------\n",
    "# kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "# clusters = kmeans.fit_predict(df_merged.T)\n",
    "\n",
    "# -------------------------------------\n",
    "# Gaussian Mixture Model (Optional)\n",
    "# -------------------------------------\n",
    "# n_clusters = 5  # Try values between 3 and 8\n",
    "# gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)\n",
    "# clusters = gmm.fit_predict(latent)\n",
    "\n",
    "# -------------------------------------\n",
    "# Spectral Biclustering (Optional)\n",
    "# -------------------------------------\n",
    "# expr_var = df_merged.var(axis=1)\n",
    "# top_genes = expr_var.sort_values(ascending=False).head(2000).index\n",
    "# df_hvg = df_merged.loc[top_genes]\n",
    "# model = SpectralBiclustering(n_clusters=5, method='log', random_state=0)\n",
    "# model.fit(df_hvg.T)\n",
    "# clusters = model.row_labels_\n",
    "\n",
    "# -------------------------------------\n",
    "# HDBSCAN Clustering (Optional)\n",
    "# -------------------------------------\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=10)\n",
    "# clusters = clusterer.fit_predict(df_zscore.T)\n",
    "\n",
    "# -------------------------------------\n",
    "# Hierarchical Clustering (Optional)\n",
    "# -------------------------------------\n",
    "# distance_matrix = pdist(X_phate, metric='euclidean')\n",
    "# Z = linkage(distance_matrix, method='ward')\n",
    "# clusters = fcluster(Z, t=5, criterion='maxclust')\n",
    "\n",
    "# -------------------------------------\n",
    "# Spectral Clustering (used here)\n",
    "# -------------------------------------\n",
    "n_clusters = 5  # Adjust as needed for desired number of subtypes\n",
    "sc = SpectralClustering(\n",
    "    n_clusters=n_clusters,\n",
    "    affinity='nearest_neighbors',     # Alternative: 'rbf'\n",
    "    n_neighbors=10,                   # 10â€“30 is common for gene expression\n",
    "    assign_labels='kmeans',           # Also try 'discretize'\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clusters = sc.fit_predict(X_phate)  # Assuming X_phate is defined earlier\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e93347cc92f8b869"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“ˆ Step 4: Visualization\n",
    "\n",
    "- 2D scatter plots (e.g., PCA) showing predicted clusters.\n",
    "- Stacked bar plots showing subtype composition within each cluster.\n",
    "- Helps interpret how well clustering corresponds to known biology.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c953d8981ae73b06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Visualize clustering results on PCA projection ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='Set1')\n",
    "plt.title(\"Unsupervised Clustering (Spectral) on Gene Expression Data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Compare predicted clusters with true PAM50 subtypes ===\n",
    "\n",
    "# Assign cluster labels to metadata\n",
    "df_zscore_meta[\"Cluster\"] = clusters\n",
    "\n",
    "# Count how many samples of each PAM50 subtype appear in each cluster\n",
    "df_counts = df_zscore_meta.groupby([\"Cluster\", \"pam50_subtype\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Compute total number of samples in each cluster\n",
    "df_totals = df_counts.groupby(\"Cluster\")[\"count\"].transform(\"sum\")\n",
    "\n",
    "# Add a proportion column for stacked bar plot\n",
    "df_counts[\"proportion\"] = df_counts[\"count\"] / df_totals\n",
    "\n",
    "# Pivot table for plotting (clusters as rows, subtypes as columns)\n",
    "pivot_df = df_counts.pivot(index=\"Cluster\", columns=\"pam50_subtype\", values=\"proportion\")\n",
    "\n",
    "# Plot stacked bar chart showing subtype composition per cluster\n",
    "pivot_df.plot(kind=\"bar\", stacked=True, figsize=(8, 5), colormap=\"Set2\")\n",
    "\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Proportion of PAM50 Subtypes within Each Cluster\")\n",
    "plt.legend(title=\"PAM50 Subtype\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75316aeac5db5125"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Š Step 5: Clustering Evaluation\n",
    "\n",
    "Several metrics are used to compare predicted clusters to true PAM50 subtypes:\n",
    "\n",
    "- **ARI** (Adjusted Rand Index)\n",
    "- **NMI** (Normalized Mutual Information)\n",
    "- **AMI** (Adjusted Mutual Information)\n",
    "- **Homogeneity / Completeness / V-measure**\n",
    "- **Silhouette Score** (higher is better)\n",
    "- **Davies-Bouldin Index** (lower is better)\n",
    "- **Hungarian Matching Accuracy** for optimal alignment of predicted and true labels\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23fdcceb42f9d1f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# === Evaluate clustering performance ===\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score, \n",
    "    normalized_mutual_info_score, \n",
    "    adjusted_mutual_info_score, \n",
    "    homogeneity_completeness_v_measure,\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score,\n",
    "    confusion_matrix, \n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Convert ground truth PAM50 labels to integers ===\n",
    "y_true = LabelEncoder().fit_transform(df_zscore_meta[\"pam50_subtype\"])  # true PAM50 subtype labels\n",
    "y_pred = clusters  # predicted cluster labels from KMeans / GMM / Spectral, etc.\n",
    "\n",
    "# === 2. External clustering evaluation metrics ===\n",
    "ari = adjusted_rand_score(y_true, y_pred)  # Adjusted Rand Index\n",
    "nmi = normalized_mutual_info_score(y_true, y_pred)  # Normalized Mutual Information\n",
    "ami = adjusted_mutual_info_score(y_true, y_pred)    # Adjusted Mutual Information\n",
    "h, c, v = homogeneity_completeness_v_measure(y_true, y_pred)  # Homogeneity, Completeness, V-measure\n",
    "\n",
    "print(f\"ARI = {ari:.3f}\")\n",
    "print(f\"NMI = {nmi:.3f}\")\n",
    "print(f\"AMI = {ami:.3f}\")\n",
    "print(f\"Homogeneity = {h:.3f}, Completeness = {c:.3f}, V-measure = {v:.3f}\")\n",
    "\n",
    "# === 3. Internal clustering evaluation metrics ===\n",
    "sil = silhouette_score(X_encoded, y_pred)  # Measures intra-cluster tightness and inter-cluster separation\n",
    "db = davies_bouldin_score(X_encoded, y_pred)  # Lower is better\n",
    "\n",
    "print(f\"Silhouette Score = {sil:.3f} (higher is better)\")\n",
    "print(f\"Davies-Bouldin Index = {db:.3f} (lower is better)\")\n",
    "\n",
    "# === 4. Cluster Label Alignment Using Hungarian Algorithm ===\n",
    "# This step is optional but useful to evaluate clustering accuracy by aligning predicted labels with true labels\n",
    "\n",
    "# Confusion matrix between true and predicted clusters\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Optimal assignment to maximize overlap using Hungarian algorithm\n",
    "row_ind, col_ind = linear_sum_assignment(-conf_mat)\n",
    "\n",
    "# Build mapping: predicted â†’ true\n",
    "mapping = {pred_label: true_label for pred_label, true_label in zip(col_ind, row_ind)}\n",
    "\n",
    "# Apply mapping to predictions\n",
    "y_pred_aligned = np.array([mapping[label] for label in y_pred])\n",
    "\n",
    "# Compute aligned accuracy\n",
    "acc = accuracy_score(y_true, y_pred_aligned)\n",
    "print(f\"Cluster Accuracy (after Hungarian alignment): {acc:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7956a6c9041bbe77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## âœ… Summary\n",
    "\n",
    "This notebook demonstrates how **unsupervised learning** and **dimensionality reduction** can be combined to uncover biologically meaningful clusters from breast cancer transcriptomic data. The recovered clusters are evaluated against **PAM50 subtypes**, providing insights into the effectiveness of each method.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c8ac45bd46a924"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
